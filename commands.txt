ssh seif@raspberrypi
python3 -m venv ~/tensorflow-env
source ~/tensorflow-env/bin/activate


python3 -m venv myenv
source myenv/bin/activate



sudo shutdown -h now

source my_face_recognition_env/bin/activate
python3 face_recognition_script.py

cd "C:\Users\sseno\OneDrive\Documents\FYP"
python noti.py


export PYTHONPATH=/usr/lib/python3/dist-packages:$PYTHONPATH
nano ~/.bashrc
export PYTHONPATH=/usr/lib/python3/dist-packages:$PYTHONPATH
source ~/.bashrc




import face_recognition
import cv2
import os
import time
import pickle
from picamera2 import Picamera2
from deepface import DeepFace  # For emotion detection

# Load images and encodings from SEIF's folder (your dataset)
def load_face_encodings(folder_path):
    encodings = []
    names = []
    for file_name in os.listdir(folder_path):
        if file_name.endswith(('jpg', 'jpeg', 'png')):
            try:
                image = face_recognition.load_image_file(os.path.join(folder_path, file_name))
                encoding = face_recognition.face_encodings(image)
                if encoding:
                    encodings.append(encoding[0])  # Assuming one face per image
                    names.append("SEIF")  # Labeling all faces as "SEIF"
            except IndexError:
                print(f"No face found in {file_name}, skipping this image.")
    return encodings, names

# Save face encodings to a file
def save_encodings_to_file(encodings, names, file_path):
    with open(file_path, 'wb') as f:
        pickle.dump((encodings, names), f)

# Load face encodings from a file
def load_encodings_from_file(file_path):
    if os.path.exists(file_path):
        with open(file_path, 'rb') as f:
            return pickle.load(f)
    else:
        return [], []

# Paths
dataset_path = "/home/seif/SEIF"
encodings_file = "/home/seif/face_encodings.pkl"

# Load existing encodings or create new ones
known_face_encodings, known_face_names = load_encodings_from_file(encodings_file)
if not known_face_encodings:
    known_face_encodings, known_face_names = load_face_encodings(dataset_path)
    save_encodings_to_file(known_face_encodings, known_face_names, encodings_file)

# Initialize Picamera2
picam2 = Picamera2()
camera_config = picam2.create_preview_configuration(main={"size": (640, 480)})
picam2.configure(camera_config)
picam2.start()

# Allow camera to warm up
time.sleep(2)

# Main loop to capture frames and detect faces
unknown_face_detected = False
new_face_encoding = None

while True:
    frame = picam2.capture_array()

    if frame is None:
        print("Error capturing frame from the camera.")
        break

    # Convert the frame to RGB format for face_recognition compatibility
    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)

    # Optionally resize the frame for faster processing
    small_frame = cv2.resize(rgb_frame, (0, 0), fx=0.5, fy=0.5)

    # Find all face locations and encodings in the current frame
    face_locations = face_recognition.face_locations(small_frame)
    face_encodings = face_recognition.face_encodings(small_frame, face_locations)

    # Loop over each face found in the live frame
    face_found = False  # Track if a known face is found
    for face_encoding, face_location in zip(face_encodings, face_locations):
        # Compute the distances between the current face and known faces
        face_distances = face_recognition.face_distance(known_face_encodings, face_encoding)
        
        # Ensure there are face distances to process
        if face_distances.size > 0:
            # Find the best match index
            best_match_index = face_distances.argmin()  # Index of the smallest distance (best match)
            best_match_distance = face_distances[best_match_index]
            
            # Only accept this as a match if it's below the threshold
            if best_match_distance < 0.4:  # Threshold for face distance
                name = known_face_names[best_match_index]

                # Log access granted for recognized names
                print(f"Access granted for {name}!")
                
                # Update the encoding for adaptive learning
                known_face_encodings[best_match_index] = face_encoding  # Update encoding
                print(f"Updated {name}'s face encodings for improved recognition.")

                # Reset unknown face detection when SEIF is found
                if name == "SEIF":
                    unknown_face_detected = False  # Reset the unknown face flag
                    face_found = True  # A known face (SEIF) has been found
            else:
                name = "Unknown"
        else:
            name = "Unknown"

        # Check if the face is unknown (above threshold or no close match)
        if name == "Unknown":
            unknown_face_detected = True
            new_face_encoding = face_encoding
            emotion_label = "N/A"  # No emotion detection for unknown faces
        else:
            # Perform emotion detection only if the face is recognized
            try:
                # Emotion analysis on the current face region
                top, right, bottom, left = face_location
                top *= 2
                right *= 2
                bottom *= 2
                left *= 2
                face_region = frame[top:bottom, left:right]
                emotion_result = DeepFace.analyze(face_region, actions=["emotion"], enforce_detection=False)
                emotion_label = emotion_result[0]["dominant_emotion"]
            except Exception as e:
                print("Error in emotion detection:", e)
                emotion_label = "Unknown"

        # Scale back up face locations since the frame was resized
        top, right, bottom, left = [v * 2 for v in face_location]

        # Display a rectangle around the face and label it
        cv2.rectangle(frame, (left, top), (right, bottom), (0, 255, 0), 2)
        cv2.putText(frame, f"{name} - {emotion_label}", (left, top - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)

    # If an unknown face is detected and no known face is found, show the prompt
    if unknown_face_detected and not face_found:
        cv2.putText(frame, "Unknown face detected! Press 'a' to authorize, 'q' to quit.", (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 0, 255), 2)

    # Show the video frame
    cv2.imshow('Video', frame)

    # Check for key presses
    key = cv2.waitKey(1) & 0xFF

    if key == ord('q'):
        break
    elif key == ord('a') and unknown_face_detected:
        new_name = input("Enter the name for the new authorized person: ")
        known_face_encodings.append(new_face_encoding)
        known_face_names.append(new_name)
        save_encodings_to_file(known_face_encodings, known_face_names, encodings_file)
        print(f"{new_name} added to the authorized list.")
        unknown_face_detected = False  # Reset after adding

# Release the camera and close windows
picam2.stop()
cv2.destroyAllWindows()


--------



import face_recognition
import cv2
import os
import time
import pickle
from picamera2 import Picamera2
from deepface import DeepFace
import numpy as np

# Paths
dataset_path = "/home/seif/SEIF"
encodings_file = "/home/seif/face_encodings.pkl"

# Function to load face encodings from the dataset
def load_face_encodings(folder_path):
    encodings = []
    names = []
    for file_name in os.listdir(folder_path):
        if file_name.endswith(('jpg', 'jpeg', 'png')):
            try:
                image = face_recognition.load_image_file(os.path.join(folder_path, file_name))
                encoding = face_recognition.face_encodings(image)
                if encoding:
                    encodings.append(encoding[0])
                    names.append("SEIF")
            except IndexError:
                print(f"No face found in {file_name}, skipping.")
    return encodings, names

# Function to save encodings to a file
def save_encodings_to_file(encodings, names, file_path):
    with open(file_path, 'wb') as f:
        pickle.dump((encodings, names), f)

# Function to load encodings from a file
def load_encodings_from_file(file_path):
    if os.path.exists(file_path):
        with open(file_path, 'rb') as f:
            return pickle.load(f)
    else:
        return [], []

# Load existing encodings or create new ones
known_face_encodings, known_face_names = load_encodings_from_file(encodings_file)
if not known_face_encodings:
    known_face_encodings, known_face_names = load_face_encodings(dataset_path)
    save_encodings_to_file(known_face_encodings, known_face_names, encodings_file)

# Initialize Picamera2
picam2 = Picamera2()
camera_config = picam2.create_preview_configuration(main={"size": (640, 480)})  # Reduced resolution
picam2.configure(camera_config)
picam2.start()
time.sleep(2)  # Allow camera to warm up

# Data Collection Variables
total_faces_detected = 0
unique_authorized_faces = set()
unique_unknown_faces = set()
total_frames_processed = 0
emotion_detection_times = []
face_detection_times = []
emotion_counts = {}
pixel_sizes = []
movements = []
previous_face_locations = []

# Frame skipping for better performance
frame_skip = 5

# Main loop
unknown_face_detected = False
new_face_encoding = None

while True:
    frame = picam2.capture_array()

    if frame is None:
        print("Error capturing frame.")
        break

    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)

    # Measure time for face detection
    start_detection_time = time.time()
    face_locations = face_recognition.face_locations(rgb_frame, model="hog")
    face_encodings = face_recognition.face_encodings(rgb_frame, face_locations)
    face_detection_times.append(time.time() - start_detection_time)

    total_frames_processed += 1

    current_movements = []

    for i, (face_encoding, face_location) in enumerate(zip(face_encodings, face_locations)):
        total_faces_detected += 1
        face_distances = face_recognition.face_distance(known_face_encodings, face_encoding)
        name = "Unknown"

        # Pixel size of the detected face
        top, right, bottom, left = face_location
        face_width = right - left
        face_height = bottom - top
        pixel_sizes.append((face_width, face_height))

        if face_distances.size > 0:
            best_match_index = face_distances.argmin()
            best_match_distance = face_distances[best_match_index]

            if best_match_distance < 0.4:
                name = known_face_names[best_match_index]
                unique_authorized_faces.add(name)
                known_face_encodings[best_match_index] = face_encoding
                unknown_face_detected = False
            else:
                unknown_face_detected = True
                new_face_encoding = face_encoding
                unique_unknown_faces.add(tuple(face_encoding))  # Use tuple as encoding is a numpy array

        # Emotion detection for every `frame_skip` frames
        emotion_label = "N/A"
        if name != "Unknown" and total_frames_processed % frame_skip == 0:
            start_emotion_time = time.time()
            try:
                face_region = rgb_frame[top:bottom, left:right]
                emotion_result = DeepFace.analyze(face_region, actions=["emotion"], enforce_detection=False)
                emotion_label = emotion_result[0]["dominant_emotion"]
                for emotion, value in emotion_result[0]["emotion"].items():
                    if emotion not in emotion_counts:
                        emotion_counts[emotion] = 0
                    emotion_counts[emotion] += 1
            except Exception as e:
                print("Error in emotion detection:", e)
            emotion_detection_times.append(time.time() - start_emotion_time)

        # Calculate movement (distance in pixels)
        current_movements.append(face_location)

        # Draw bounding boxes and display name/emotion
        cv2.rectangle(frame, (left, top), (right, bottom), (0, 255, 0), 2)
        cv2.putText(frame, f"{name} - {emotion_label}", (left, top - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)

    if previous_face_locations:
        for prev_loc, curr_loc in zip(previous_face_locations, current_movements):
            distance = np.linalg.norm(np.array(prev_loc) - np.array(curr_loc))
            movements.append(distance)

    previous_face_locations = current_movements

    if unknown_face_detected:
        cv2.putText(frame, "Unknown face detected! Press 'a' to authorize, 'q' to quit.", (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 0, 255), 2)

    if total_frames_processed % frame_skip == 0:  # Show frame only for skipped intervals
        cv2.imshow('Face Recognition', frame)

    key = cv2.waitKey(1) & 0xFF
    if key == ord('q'):
        break
    elif key == ord('a') and unknown_face_detected:
        new_name = input("Enter the name for the new authorized person: ")
        known_face_encodings.append(new_face_encoding)
        known_face_names.append(new_name)
        save_encodings_to_file(known_face_encodings, known_face_names, encodings_file)
        unknown_face_detected = False

picam2.stop()
cv2.destroyAllWindows()

# Results Calculation
average_emotion_detection_time = sum(emotion_detection_times) / len(emotion_detection_times) if emotion_detection_times else 0
average_face_detection_time = sum(face_detection_times) / len(face_detection_times) if face_detection_times else 0
total_unique_authorized_faces = len(unique_authorized_faces)
total_unique_unknown_faces = len(unique_unknown_faces)
average_movement = sum(movements) / len(movements) if movements else 0

# Save Results to File
with open("results.txt", "w") as f:
    f.write("\n===== Script Results =====\n")
    f.write(f"Total Frames Processed: {total_frames_processed}\n")
    f.write(f"Total Faces Detected: {total_faces_detected}\n")
    f.write(f"Unique Authorized Faces Detected: {total_unique_authorized_faces}\n")
    f.write(f"Unique Unknown Faces Detected: {total_unique_unknown_faces}\n")
    f.write(f"Average Face Detection Time: {average_face_detection_time * 1000:.2f} ms\n")
    f.write(f"Average Emotion Detection Time: {average_emotion_detection_time * 1000:.2f} ms\n")
    f.write(f"Pixel Sizes: {pixel_sizes}\n")
    f.write(f"Number of Emotions Detected: {emotion_counts}\n")
    f.write(f"Average Movement Between Frames: {average_movement:.2f} pixels\n")
